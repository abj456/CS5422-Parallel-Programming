---
title: HW3 All-Pairs Shortest Path

---

# HW3 All-Pairs Shortest Path
> [name=蔡品棠, 112062525]
## Implementation
1. Which algorithm do you choose in hw3-1?
    * 我採用**Blocked Floyd-Warshall** Algorithm。
    * 實作時以OpenMP將task做平行處理。
        * ![image](https://hackmd.io/_uploads/BJrGadNSJx.png)
        * 這邊針對phase 2和phase 3，因為phase 2在做藍色部分的計算以及phase 3做紅色部分的計算時，每個block的data都是independent的。因此可直接以`#pragma omp parallel for`做平行。
            ```cpp
            void cal(int B, int Round, int block_start_x, int block_start_y, int block_width, int block_height);

            void block_FW(int B) {
                int round = ceil(n, B);

                for (int r = 0; r < round; ++r) {
                    printf("%d %d\n", r, round);
                    fflush(stdout);
                    /* Phase 1*/
                    cal(B, r, r, r, 1, 1);

                    /* Phase 2*/
                    #pragma omp parallel for num_threads(NUM_THREADS) schedule(dynamic)
                    for(int i = 0; i < round; ++i) {
                        if(i != r) {
                            cal(B, r, r, i, 1, 1);
                            cal(B, r, i, r, 1, 1);
                        }
                    }

                    /* Phase 3*/
                    #pragma omp parallel for num_threads(NUM_THREADS) collapse(2) schedule(dynamic)
                    for(int i = 0; i < round; ++i){
                        for(int j = 0; j < round; ++j) {
                            if(i != r && j != r) {
                                cal(B, r, i, j, 1, 1);
                            }
                        }
                    }
                }
            }
            ```
        * 而Blocked Floyd-Warshall function則沒有變動太多，基本遵照sequential版本的`cal()`。而最外兩層的for loop其實可以拿掉，因為在`block_FW()`中已經做到分塊的動作了，每次function call要算的block數量都是1。這邊不拿主要是想說可以讓兩邊的cal大概一致，若要像sequential版本傳入多個數量的blocks也能讓function運作，維持function的通用性。
            ```cpp
            void cal(
            int B, int Round, int block_start_x, int block_start_y, int block_width, int block_height) {
                int block_end_x = block_start_x + block_height;
                int block_end_y = block_start_y + block_width;
                int k_start = Round * B, k_end = std::min((Round + 1) * B, n);

                //這邊兩層可以拿掉
                for (int b_i = block_start_x; b_i < block_end_x; ++b_i) {
                    for (int b_j = block_start_y; b_j < block_end_y; ++b_j) {
                        // To calculate B*B elements in the block (b_i, b_j)
                        // For each block, it need to compute B times
                        int block_internal_start_x = b_i * B;
                        int block_internal_end_x = std::min((b_i + 1) * B, n);
                        int block_internal_start_y = b_j * B;
                        int block_internal_end_y = std::min((b_j + 1) * B, n);

                        // if (block_internal_end_x > n) block_internal_end_x = n;
                        // if (block_internal_end_y > n) block_internal_end_y = n;

                        for (int k = k_start; k < k_end; ++k) {
                            // To calculate original index of elements in the block (b_i, b_j)
                            // For instance, original index of (0,0) in block (1,2) is (2,5) for V=6,B=2
                            for (int i = block_internal_start_x; i < block_internal_end_x; ++i) {
                                for (int j = block_internal_start_y; j < block_internal_end_y; ++j) {
                                    if (Dist[i][k] + Dist[k][j] < Dist[i][j]) {
                                        Dist[i][j] = Dist[i][k] + Dist[k][j];
                                    }
                                }
                            }
                        }
                    }
                }
            }
            ```
2. How do you divide your data in hw3-2, hw3-3?
    * BlockFactor設為64，將data切為64 \* 64的大小，而每個block便會負責這樣大小的data。。
    * 由於block中的thread數量上限為32 \* 32，因此每個thread會負責運算(64 \* 64) / (32 \* 32) = 4個data。
    * 在kernel function中，要取每個block或每個thread負責的data只需算出基底的`global_idx`，以此去fetch其他三個address就好。底下以phase1為例:
        ```cpp
        #define addr(V, i, j) ((i) * V + (j))
        __global__ void cal_phase1(int n, int *Dist, int B, int Round) {

            int thread_i = threadIdx.y;
            int thread_j = threadIdx.x;
            int global_i = Round * BF + thread_i;
            int global_j = Round * BF + thread_j;

            __shared__ int shared_block[BF][BF];
            int global_idx = addr(n, global_i, global_j);
            shared_block[thread_i][thread_j] = Dist[global_idx];
            shared_block[thread_i][thread_j + HALF_BF] = Dist[global_idx + HALF_BF];
            shared_block[thread_i + HALF_BF][thread_j] = Dist[global_idx + HALF_BF * n];
            shared_block[thread_i + HALF_BF][thread_j + HALF_BF] = Dist[global_idx + HALF_BF * (n + 1)];
            __syncthreads();

            PRAGMA_UNROLL(BF)
            for(int k = 0; k < BF; ++k) {
                shared_block[thread_i][thread_j] = min(shared_block[thread_i][thread_j], 
                                                      (shared_block[thread_i][k] + shared_block[k][thread_j]));

                shared_block[thread_i][thread_j + HALF_BF] = min(shared_block[thread_i][thread_j + HALF_BF],
                                                                (shared_block[thread_i][k] + shared_block[k][thread_j + HALF_BF]));

                shared_block[thread_i + HALF_BF][thread_j] = min(shared_block[thread_i + HALF_BF][thread_j],
                                                                (shared_block[thread_i + HALF_BF][k] + shared_block[k][thread_j]));

                shared_block[thread_i + HALF_BF][thread_j + HALF_BF] = min(shared_block[thread_i + HALF_BF][thread_j + HALF_BF],
                                                                          (shared_block[thread_i + HALF_BF][k] + shared_block[k][thread_j + HALF_BF]));
                // __syncthreads();
            }

            Dist[global_idx] = shared_block[thread_i][thread_j];
            Dist[global_idx + HALF_BF] = shared_block[thread_i][thread_j + HALF_BF];
            Dist[global_idx + HALF_BF * n] = shared_block[thread_i + HALF_BF][thread_j];
            Dist[global_idx + HALF_BF * (n + 1)] = shared_block[thread_i + HALF_BF][thread_j + HALF_BF];
        }
        ```
    * hw3-3: phase 1 & phase 2和hw3-2一樣，在phase 3才因為兩張GPU有分工各自計算的部分而有些調整。具體會在hw3-3的implementation說明。
3. What’s your configuration in hw3-2, hw3-3? And why? (e.g. blocking factor, #blocks, #threads)
    * *因hw3-3*
    * blocking factor: 設為64。在phase3中，需要3個int型態的shared memory，而`sharedMemPerBlock`為49152 bytes。因此 $Blocking\ Factor = \sqrt{49152 \div sizeof(int) \div 3} = 64$ 的情況下可以完全不浪費shared memory的空間。
    * blocks: 
        * 在phase 1中，只需1個gpu block計算pivot block。
        * phase 2因為可以讓每個block一起計算對應順序的row以及column block (如上圖b，pivot block右邊的pivot column和下面的pivot row可以在同個block一起算)，這樣只需 $round = number\ of\ vertex \div Blocking\ Factor$ 個blocks。
        * phase 3需要計算的是pivot block、pivot column以及pivot row以外的blocks，$number\ of\ blocks = (round - 1)^2$。但為了方便實作，實際開啟phase 3的kernel function時，我依然開了$round^2$個blocks，並把不用計算的blocks直接return不做運算即可。
        * hw3-3: 概念跟上面hw3-2差不多，不過因為使用OpenMP來控制兩張GPU的運算，phase 3分配到的blocks數也有相應調整。具體一樣會在hw3-3的implementation提到。
    * threads: 因`maxThreasPerBlock`為1024，因此就設為32 * 32的threads數量。
        * hw3-3: 一樣使用1024個threads。
    * padding: 在input function裡，為了減少kernel function中branch的數量來提升效能，我會在vertex數量padding為64的倍數。只要在output時，每個row只輸出原有的n個vertex的elements就好。
4. How do you implement the communication in hw3-3?
    * 同樣於hw3-3 implementation說明。
5. Briefly describe your implementations in diagrams, figures or sentences.
    ### Single GPU (hw3-2)
    * phase 1: 這邊只做pivot block的計算，所以只開一個`__shared__ int shared_block[BF][BF]`來做，而這部分和原本的Floyd-Warshall非常相似(如Ans 2)。不過原以為for loop內做運算時會有data dependency的問題，但judge時試過把`__syncthreads()`拿掉發現不影響正確性，因此這邊為了效能就註解掉了。
    * phase 2: 此階段是基於phase 1計算出的pivot block進一步去處理與該block相同column、相同row的其他blocks。**shared memory**則因為需要pivot block輔助pivot column & pivot row，會需要三塊$BF * BF$ size的shared memory。
        ```cpp
        __global__ void cal_phase2(int n, int *Dist, int B, int Round) {
            if(blockIdx.x == Round) return;

            int thread_i = threadIdx.y;
            int thread_j = threadIdx.x;

            // real index in Dist
            int global_i = Round * BF + thread_i;
            int global_j = Round * BF + thread_j;
            // horizontal computation -> i fixed
            int hz_i = global_i;
            int hz_j = blockIdx.x * BF + thread_j;
            // vertical computation -> j fixed
            int vt_i = blockIdx.x * BF + thread_i;
            int vt_j = global_j;

            __shared__ int shared_pivot[BF][BF];
            __shared__ int shared_hz[BF][BF];
            __shared__ int shared_vt[BF][BF];

            int global_idx = addr(n, global_i, global_j);
            shared_pivot[thread_i][thread_j] = Dist[global_idx];
            shared_pivot[thread_i][thread_j + HALF_BF] = Dist[global_idx + HALF_BF];
            shared_pivot[thread_i + HALF_BF][thread_j] = Dist[global_idx + HALF_BF * n];
            shared_pivot[thread_i + HALF_BF][thread_j + HALF_BF] = Dist[global_idx + HALF_BF * (n + 1)];

            int hz_idx = addr(n, hz_i, hz_j);
            shared_hz[thread_i][thread_j] = Dist[hz_idx];
            shared_hz[thread_i][thread_j + HALF_BF] = Dist[hz_idx + HALF_BF];
            shared_hz[thread_i + HALF_BF][thread_j] = Dist[hz_idx + HALF_BF * n];
            shared_hz[thread_i + HALF_BF][thread_j + HALF_BF] = Dist[hz_idx + HALF_BF * (n + 1)];

            int vt_idx = addr(n, vt_i, vt_j);
            shared_vt[thread_i][thread_j] = Dist[vt_idx];
            shared_vt[thread_i][thread_j + HALF_BF] = Dist[vt_idx + HALF_BF];
            shared_vt[thread_i + HALF_BF][thread_j] = Dist[vt_idx + HALF_BF * n];
            shared_vt[thread_i + HALF_BF][thread_j + HALF_BF] = Dist[vt_idx + HALF_BF * (n + 1)];

            __syncthreads();

            // #pragma unroll BF
            PRAGMA_UNROLL(BF)
            for(int k = 0; k < BF; ++k) {
                shared_hz[thread_i][thread_j] = min(shared_hz[thread_i][thread_j], 
                                                    shared_pivot[thread_i][k] + shared_hz[k][thread_j]);
                shared_hz[thread_i][thread_j + HALF_BF] = min(shared_hz[thread_i][thread_j + HALF_BF], 
                                                              shared_pivot[thread_i][k] + shared_hz[k][thread_j + HALF_BF]);
                shared_hz[thread_i + HALF_BF][thread_j] = min(shared_hz[thread_i + HALF_BF][thread_j], 
                                                              shared_pivot[thread_i + HALF_BF][k] + shared_hz[k][thread_j]);
                shared_hz[thread_i + HALF_BF][thread_j + HALF_BF] = min(shared_hz[thread_i + HALF_BF][thread_j + HALF_BF], 
                                                                        shared_pivot[thread_i + HALF_BF][k] + shared_hz[k][thread_j + HALF_BF]);

                shared_vt[thread_i][thread_j] = min(shared_vt[thread_i][thread_j],
                                                    shared_vt[thread_i][k] + shared_pivot[k][thread_j]);
                shared_vt[thread_i][thread_j + HALF_BF] = min(shared_vt[thread_i][thread_j + HALF_BF],
                                                              shared_vt[thread_i][k] + shared_pivot[k][thread_j + HALF_BF]);
                shared_vt[thread_i + HALF_BF][thread_j] = min(shared_vt[thread_i + HALF_BF][thread_j],
                                                              shared_vt[thread_i + HALF_BF][k] + shared_pivot[k][thread_j]);
                shared_vt[thread_i + HALF_BF][thread_j + HALF_BF] = min(shared_vt[thread_i + HALF_BF][thread_j + HALF_BF],
                                                                        shared_vt[thread_i + HALF_BF][k] + shared_pivot[k][thread_j + HALF_BF]);
                // __syncthreads();
            }

            Dist[hz_idx] = shared_hz[thread_i][thread_j];
            Dist[hz_idx + HALF_BF] = shared_hz[thread_i][thread_j + HALF_BF];
            Dist[hz_idx + HALF_BF * n] = shared_hz[thread_i + HALF_BF][thread_j];
            Dist[hz_idx + HALF_BF * (n + 1)] = shared_hz[thread_i + HALF_BF][thread_j + HALF_BF];

            Dist[vt_idx] = shared_vt[thread_i][thread_j];
            Dist[vt_idx + HALF_BF] = shared_vt[thread_i][thread_j + HALF_BF];
            Dist[vt_idx + HALF_BF * n] = shared_vt[thread_i + HALF_BF][thread_j];
            Dist[vt_idx + HALF_BF * (n + 1)] = shared_vt[thread_i + HALF_BF][thread_j + HALF_BF];
        }
        ```
    * phase 3: 這邊要用到phase 2計算出的pivot column和pivot row輔助計算出當前blockIdx.x & blockIdx.y對應到Dist的區塊。因此大致上和phase 2的code結構差不多，只是在for loop計算時反過來，要用shared_hz和shared_vt來算shared_pivot。
        ```cpp
        __global__ void cal_phase3(int n, int *Dist, int Round) {
            // skip pivot, pivot hz, pivot vt
            if(blockIdx.x == Round || blockIdx.y == Round) return; 

            __shared__ int block[BF][BF];
            __shared__ int vt[BF][BF];
            __shared__ int hz[BF][BF];

            int thread_i = threadIdx.y;
            int thread_j = threadIdx.x;
            int b_i = blockIdx.y;
            int b_j = blockIdx.x;

            /* idx of block waiting to be computed */
            int block_i = b_i * BF + thread_i;
            int block_j = b_j * BF + thread_j;

            /* idx of pivot hz block */
            int hz_i = Round * BF + thread_i; // row fixed, col changed
            // int hz_j = block_j;

            /* idx of pivot vt block */
            // int vt_i = block_i;
            int vt_j = Round * BF + thread_j; // row changed, col fixed


            int block_idx = addr(n, block_i, block_j);
            block[thread_i][thread_j] = Dist[block_idx];
            block[thread_i][thread_j + HALF_BF] = Dist[block_idx + HALF_BF];
            block[thread_i + HALF_BF][thread_j] = Dist[block_idx + HALF_BF * n];
            block[thread_i + HALF_BF][thread_j + HALF_BF] = Dist[block_idx + HALF_BF * (n + 1)];

            // row changed, col fixed
            int vt_idx = addr(n, block_i, vt_j);
            vt[thread_i][thread_j] = Dist[vt_idx];
            vt[thread_i][thread_j + HALF_BF] = Dist[vt_idx + HALF_BF];
            vt[thread_i + HALF_BF][thread_j] = Dist[vt_idx + HALF_BF * n];
            vt[thread_i + HALF_BF][thread_j + HALF_BF] = Dist[vt_idx + HALF_BF * (n + 1)];

            // row fixed, col changed
            int hz_idx = addr(n, hz_i, block_j);
            hz[thread_i][thread_j] = Dist[hz_idx];
            hz[thread_i][thread_j + HALF_BF] = Dist[hz_idx + HALF_BF];
            hz[thread_i + HALF_BF][thread_j] = Dist[hz_idx + HALF_BF * n];
            hz[thread_i + HALF_BF][thread_j + HALF_BF] = Dist[hz_idx + HALF_BF * (n + 1)];

            __syncthreads();

            // #pragma unroll BF
            PRAGMA_UNROLL(BF)
            for(int k = 0; k < BF; ++k) {
                block[thread_i][thread_j] = min(block[thread_i][thread_j], 
                                                vt[thread_i][k] + hz[k][thread_j]);

                block[thread_i][thread_j + HALF_BF] = min(block[thread_i][thread_j + HALF_BF], 
                                                          vt[thread_i][k] + hz[k][thread_j + HALF_BF]);

                block[thread_i + HALF_BF][thread_j] = min(block[thread_i + HALF_BF][thread_j], 
                                                          vt[thread_i + HALF_BF][k] + hz[k][thread_j]);

                block[thread_i + HALF_BF][thread_j + HALF_BF] = min(block[thread_i + HALF_BF][thread_j + HALF_BF], 
                                                                    vt[thread_i + HALF_BF][k] + hz[k][thread_j + HALF_BF]);
                // __syncthreads();
            }

            Dist[block_idx] = block[thread_i][thread_j];
            Dist[block_idx + HALF_BF] = block[thread_i][thread_j + HALF_BF];
            Dist[block_idx + HALF_BF * n] = block[thread_i + HALF_BF][thread_j];
            Dist[block_idx + HALF_BF * (n + 1)] = block[thread_i + HALF_BF][thread_j + HALF_BF];
        }
        ```
    ### Multi-GPU (hw3-3)
    ```cpp
    void block_FW(int n, int B, int **Dist) {
    
        // int round = ceil(n, B);
        int round = n / BF;
        dim3 threadsPerBlock(NUM_THREADS, NUM_THREADS);
        dim3 blocks(1, round);

        #pragma omp parallel num_threads(2)
        {
            unsigned int cpu_tid = omp_get_thread_num();
            unsigned int half_round = round / 2;
            unsigned int row_offset = (cpu_tid) ? half_round : 0;
            unsigned int div_row = (cpu_tid) ? (round - half_round) : half_round;

            cudaSetDevice(cpu_tid);
            cudaMalloc(&Dist[cpu_tid], dist_size);
            assert(Dist[cpu_tid] != nullptr);
            cudaMemcpy(Dist[cpu_tid], Dist_host, dist_size, cudaMemcpyHostToDevice);
            dim3 blocksPerGrid(round, div_row);

            for(int r = 0; r < round; ++r) {
                if(r >= row_offset && r < row_offset + div_row) {
                    cudaMemcpy(Dist[!cpu_tid] + r * BF * n, Dist[cpu_tid] + r * BF * n, BF * n * sizeof(int), cudaMemcpyDeviceToDevice);
                }
                #pragma omp barrier

                /* Phase 1*/
                cal_phase1<<<1, threadsPerBlock>>>(n, Dist[cpu_tid], BF, r);
                /* Phase 2*/
                cal_phase2<<<round, threadsPerBlock>>>(n, Dist[cpu_tid], BF, r);
                /* Phase 3*/
                cal_phase3<<<blocksPerGrid, threadsPerBlock>>>(n, Dist[cpu_tid], r, row_offset);
            }
            cudaMemcpy(Dist_host + row_offset * BF * n, Dist[cpu_tid] + (row_offset * BF * n), 
                       div_row * BF * n * sizeof(int), cudaMemcpyDeviceToHost);
            #pragma omp barrier

            cudaFree(Dist[cpu_tid]);
        }
    }
    ```
    * **Control of Multi-GPU with Multi-thread**: 為了控制兩張GPU，我使用OpenMP來讓兩個thread去控制其對應thread id的device。
    * **data division & blocks**: phase 1 & 2的切法與hw3-2相同。在phase 3中，為了讓GPU 0計算上半部分，GPU 1計算下半部分的matrix，會把row切割成一半。因此可以看到`blocksPerGrid`被設為`(round, div_row)`。
    * phase 1 & 2: 與hw3-2相同。為了方便實作以及減少溝通成本，在兩邊的thread (GPU 0 & 1)會做同樣的計算。
    * phase 3 & communication: 在phase 3中，改動到的地方其實只有如何用row_offset變數讓各自的GPU去取到正確位置的data。
        * communication的部分則是在每個round開始計算前，如果該round對應的整個block row data是自己這個device負責的，就把這些data傳給對方。並於最後計算完再做一次synchronization。

## Profiling Results (hw3-2)
* 這邊我選用p11k1這個測資。而其中執行時間最久與計算量最大的kernel為`cal_phase3()`。
    | Metric   | Min      | Max      | Avg    |
    | -------- | -------- | -------- | ------ |
    | achieved_occupancy | 0.948153 | 0.950388 | 0.949150 |
    | sm_efficiency      | 99.79%   | 99.92%   | 99.90%   |
    | shared_load_throughput  | 2934.1GB/s | 3187.7GB/s   | 3112.2GB/s |
    | shared_store_throughput | 221.83GB/s | 253.19GB/s   | 248.50GB/s |
    | gld_throughput | 18.519GB/s | 18.769GB/s | 18.619GB/s |
    | gst_throughput | 61.859GB/s | 63.057GB/s | 62.564GB/s |


## Experiment & Analysis
* System Spec: **apollo-gpu**
* Blocking Factor (hw3-2)
    * 測資皆使用p11k1。
    * Integer GOPS
        * 測量方式為nvprof解析出的inst_integer除以nvprof抓出的`cal_phase3()`的kernel執行時間。
        * 由圖表可見，BF越大時Integer GOPS也越大。結合底下圖表可推測出是因為減少了global memory access數量，讓大部分運算都從shared memory區塊讀寫，因此運算量也大幅上升。
        * ![hw3 computation performance](https://hackmd.io/_uploads/rkwV15rryl.png)
    * global memory bandwidth
        * 以nvprof取得gld/st_throughput。
        * BF越大則global memory bandwidth越小。原因同上，global memory acccess數量減少了。
        * ![hw3 global bw](https://hackmd.io/_uploads/Syn1UqSB1g.png)
    * shared memory bandwidth
        * 以nvprof取得shared_load/store_throughput。
        * BF越大則shared memory bandwidth越大。原因同上，global memory acccess數量減少，shared memory access增加。
        * ![hw3 shared bw](https://hackmd.io/_uploads/r14g89HB1x.png)
* Optimization (hw3-2)
    * 這邊的threads與blocks數量皆相同。
    * GPU Baseline與Shared Memory所使用BF皆為32。
        * 光使用shared memory就有2.3倍多的加速。
    * 將BF換成64後進一步快了接近2倍。
    * 註解掉for loop內的`__syncthreads()`後更是優化了快要4倍。
    * ![hw3 Performance Optimization](https://hackmd.io/_uploads/ByJPniBSJg.png)
* Weak scalability (hw3-3)
    * 要計算weak scalability，2 GPUs的vertex數要是1 GPU的$\sqrt{2}$倍。因此選擇p12k1與p17k1來比較。
    * 由圖中可看出weak scalability的效果不彰，原因可能是當input size增加時，兩張GPU之間的同步成本也會有所增幅，連帶影響Computing與IO的部分。
    * ![hw3 Weak Scalability](https://hackmd.io/_uploads/BktR23SBJx.png)
* Time Distribution (hw3-2)
    * 這邊的實驗使用不同vertex數量的testcase來比較，並使用nvprof取得cudaMemCopy以及GPU kernel的Computing Time。IO則以hw4提供的`getTimeStamp()`來計算。
    * 從圖表可見到隨著vertex數增加，IO和Computing Time皆明顯增加，而cudaMemCopy則不是主要的bottleneck。
    * ![hw3 Time Distribution](https://hackmd.io/_uploads/SJ4yahrByl.png)
## Experiment on AMD GPU
* 因AMD GPU沒有nvprof用於分析，加上不是這次作業的重點，這邊就擷取兩邊judge內的時間差。
* 在`make hw3-2-amd`時，發現不能像NVIDIA GPU一樣省略for loop內的`__syncthreads()`，因此底下兩段judge結果會以同樣的程式架構(加回`__syncthreads()`)去比較。
* 可以發現NVIDIA GPU確實有因為`__syncthreads()`產生synchronization成本，但AMD GPU跑出的時間卻和N GPU拿掉for loop內`__syncthreads()`的時間差不多。因此個人覺得AMD GPU或許有針對threads之間的同步做更多的優化。
* 由於hw3-3-amd在跑c02.1和c03.1兩個測資會有wrong answer，這邊就不特別做hw3-3間的比較。
* hw3-2-judge:
    ```bash
    pp24s058@apollo-nv-test:~/CS5422-Parallel-Programming/hw3/hw3-2$ hw3-2-judge
    Looking for hw3-2.cu: OK
    Looking for Makefile: OK
    Running: /usr/bin/make -C /share/judge_dir/.judge_exe.3768611251 hw3-2
    make: Entering directory '/share/judge_dir/.judge_exe.3768611251'
    nvcc -std=c++11 -O3 -Xptxas="-v" -arch=sm_61 -Xcompiler -fopenmp -lm -o hw3-2 hw3-2.cu
    ptxas info    : 0 bytes gmem
    ptxas info    : Compiling entry function '_Z10cal_phase3iPiii' for 'sm_61'
    ptxas info    : Function properties for _Z10cal_phase3iPiii
        0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
    ptxas info    : Used 27 registers, used 1 barriers, 49152 bytes smem, 344 bytes cmem[0]
    ptxas info    : Compiling entry function '_Z10cal_phase2iPiii' for 'sm_61'
    ptxas info    : Function properties for _Z10cal_phase2iPiii
        0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
    ptxas info    : Used 27 registers, used 1 barriers, 49152 bytes smem, 344 bytes cmem[0]
    ptxas info    : Compiling entry function '_Z10cal_phase1iPiii' for 'sm_61'
    ptxas info    : Function properties for _Z10cal_phase1iPiii
        0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
    ptxas info    : Used 15 registers, used 1 barriers, 16384 bytes smem, 344 bytes cmem[0]
    make: Leaving directory '/share/judge_dir/.judge_exe.3768611251'
    c03.1    0.21   accepted
    c04.1    0.21   accepted
    c05.1    0.21   accepted
    c06.1    0.22   accepted
    c07.1    0.22   accepted
    c08.1    0.22   accepted
    c09.1    0.22   accepted
    c10.1    0.21   accepted
    c01.1    0.26   accepted
    c02.1    0.21   accepted
    c11.1    0.21   accepted
    c12.1    0.21   accepted
    c13.1    0.22   accepted
    c14.1    0.22   accepted
    c15.1    0.22   accepted
    c16.1    0.32   accepted
    c17.1    0.37   accepted
    c18.1    0.57   accepted
    c19.1    0.57   accepted
    c20.1    1.22   accepted
    p11k1    4.58   accepted
    c21.1    1.42   accepted
    p12k1    6.03   accepted
    p14k1    9.39   accepted
    p13k1    7.49   accepted
    p15k1   11.90   accepted
    p16k1   14.66   accepted
    p17k1   16.77   accepted
    p18k1   19.37   accepted
    p20k1   26.95   accepted
    p19k1   22.78   accepted
    p21k1   30.60   time limit exceeded:  {timeout}
    p22k1   34.06   time limit exceeded:  {timeout}
    p23k1   40.00   time limit exceeded:  {timeout}
    p24k1   40.00   time limit exceeded:  {timeout}
    p25k1   40.00   time limit exceeded:  {timeout}
    p26k1   40.00   time limit exceeded:  {timeout}
    p27k1   40.00   time limit exceeded:  {timeout}
    p28k1   40.00   time limit exceeded:  {timeout}
    p29k1   40.00   time limit exceeded:  {timeout}
    p30k1   40.00   time limit exceeded:  {timeout}
    ```
* hw3-2-amd-judge:
    ```bash
    pp24s058@apollo-login:~/CS5422-Parallel-Programming/hw3/hw3-2$ hw3-2-amd-judge
    Looking for hw3-2.hip: OK
    Looking for Makefile: OK
    Running: /usr/bin/make -C /share/judge_dir/.judge_exe.244934801 hw3-2-amd
    make: Entering directory '/share/judge_dir/.judge_exe.244934801'
    hipcc -std=c++11 -O3 --offload-arch=gfx90a -Xcompiler -fopenmp -lm -o hw3-2-amd hw3-2.hip
    clang++: warning: argument unused during compilation: '-Xcompiler' [-Wunused-command-line-argument]
    make: Leaving directory '/share/judge_dir/.judge_exe.244934801'
    c02.1    0.36   accepted
    c01.1    0.42   accepted
    c03.1    0.36   accepted
    c04.1    0.37   accepted
    c05.1    0.42   accepted
    c06.1    0.36   accepted
    c08.1    0.36   accepted
    c07.1    0.42   accepted
    c09.1    0.36   accepted
    c10.1    0.36   accepted
    c11.1    0.42   accepted
    c12.1    0.36   accepted
    c13.1    0.42   accepted
    c14.1    0.42   accepted
    c15.1    0.41   accepted
    c16.1    0.41   accepted
    c17.1    0.47   accepted
    c19.1    0.47   accepted
    c20.1    1.17   accepted
    c21.1    1.07   accepted
    c18.1    0.57   accepted
    p11k1    2.12   accepted
    p12k1    2.57   accepted
    p13k1    3.02   accepted
    p14k1    3.67   accepted
    p15k1    4.83   accepted
    p16k1    5.23   accepted
    p17k1    6.39   accepted
    p18k1    6.93   accepted
    p19k1    8.09   accepted
    p20k1    9.09   accepted
    p21k1   10.60   accepted
    p22k1   11.69   accepted
    p23k1   13.61   accepted
    p24k1   15.61   accepted
    p25k1   17.01   accepted
    p26k1   18.88   accepted
    p27k1   21.64   accepted
    p28k1   23.34   accepted
    p29k1   26.04   accepted
    p30k1   28.61   accepted
    p31k1   31.70   time limit exceeded:  {timeout}
    ```
* hw3-3-amd: 
    ```sh
    pp24s058@apollo-login:~/CS5422-Parallel-Programming/hw3/hw3-3$ hw3-3-amd-judge 
    Looking for hw3-3.hip: OK
    Looking for Makefile: OK
    Running: /usr/bin/make -C /share/judge_dir/.judge_exe.317174051 hw3-3-amd
    make: Entering directory '/share/judge_dir/.judge_exe.317174051'
    hipcc -std=c++11 -O3 --offload-arch=gfx90a -fopenmp -lm -o hw3-3-amd hw3-3.hip
    make: Leaving directory '/share/judge_dir/.judge_exe.317174051'
    c02.1    0.62   wrong answer: 63518a4c87baaf897febf70dcd706493dc9ba847cd87281fa4c5110e83ac8f14,19000
    c03.1    0.62   wrong answer: 0aaca0f566afeea0c4d92561d3897080023f77ba36bbd76d7e67e4c57ab4c167,3ce9c4
    c01.1    0.62   accepted
    c04.1    1.27   accepted
    c05.1    1.62   accepted
    c06.1   32.45   accepted
    c07.1   45.38   accepted
    Removing temporary directory /share/judge_dir/.judge_exe.317174051
    Scoreboard: created {5 81.33}
    ```
## Experience & Conclusion
* What have you learned from this homework?
    * 對GPU的軟硬體架構更加了解，並切身體會到threadIdx.x與threadIdx.y是有其物理意義在的。
    * GPU coding的切入點&想法與以往寫CPU sequential code很不一樣，需要時刻去思考怎麼把sequential code的for loop對應至parallel code，才不會兩邊fetch到不同的data。
    * 因為GPU parallel code幾乎不可能一次到位寫完，其撰寫流程也因此較為冗長。大致上為: 先寫出GPU版本的sequential code->轉為thread parallelizing的版本->加入block做到更好的優化。