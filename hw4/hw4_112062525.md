---
title: HW4 Flash Attention

---

# HW4 Flash Attention
> [name=è”¡å“æ£ , 112062525]
* å»ºè­°ä»¥hackmdé–‹å•Ÿï¼Œç¨‹å¼ç¢¼å€å¡Šæ‰èƒ½é¡¯ç¤ºè¡Œæ•¸ã€‚
## Implementation
1. Describe how you implemented the FlashAttention forward pass using CUDA. Mention the algorithm's key steps, such as matrix blocking, SRAM usage, and how intermediate results like scaling factors (â„“ and ğ‘š) were calculated.
    ```cpp=
    int br = BR, bc = BC;
    int tr = N / br, tc = N / bc;
    dim3 blocksPerGrid(1, tr);
    dim3 threadsPerBlock(bc, br);
    size_t sizeof_kj_vj_qi_oi = (bc * d * 2 + br * d * 2) * sizeof(float);
    printf("size of kj_vj_qi_oi_oitmp = %lu\n", sizeof_kj_vj_qi_oi);
    printf("size of static shared mem = %lu\n", sizeof(float) * (BR * 6 + BR * BC * 2));

    double start, end;
    start = getTimeStamp();

    for (int i = 0; i < B; i++) {
        cudaMemset(l, 0x00, N * sizeof(float));
        cudaMemset(m, FLT_MIN, N * sizeof(float));
        
        for(int j = 0; j < tc; ++j)
            gpu_flash_attn<<<blocksPerGrid, threadsPerBlock, sizeof_kj_vj_qi_oi>>>(
                l, m, N, d, j,
                Q_gpu + (i * N * d), 
                K_gpu + (i * N * d), 
                V_gpu + (i * N * d), 
                O_gpu + (i * N * d)
            );
    }
    
    __global__ void gpu_flash_attn(float *l, float *m, int N, int d, int j,
                                   float *q, float *k, float *v, float *o
                                   ) {
        extern __shared__ float s_mem[];
        float *kj = s_mem;
        float *vj = kj + BC * d;
        float *qi = vj + BC * d;
        float *oi = qi + BR * d;

        __shared__ float li[BR];
        __shared__ float mi[BR];
        __shared__ float sij[BR * BC];
        __shared__ float pij[BR * BC];
        __shared__ float mij[BR];
        __shared__ float lij[BR];
        __shared__ float mi_new[BR];
        __shared__ float li_new[BR];

        int thd_i = threadIdx.y;
        int thd_j = threadIdx.x;

        int row = blockIdx.y * BR + thd_i;

        int d_stride = d / BR;
        #pragma unroll
        for(int idx = 0; idx < d_stride; ++idx) {    
            kj[thd_j * d + idx * BR + thd_i] = k[j * BC * d + thd_j * d + idx * BR + thd_i];
            vj[thd_j * d + idx * BR + thd_i] = v[j * BC * d + thd_j * d + idx * BR + thd_i];
        }

        d_stride = d / BC;
        #pragma unroll
        for(int idx = 0; idx < d_stride; ++idx) {
            qi[thd_i * d + idx * BC + thd_j] = q[row * d + idx * BC + thd_j];
            oi[thd_i * d + idx * BC + thd_j] = o[row * d + idx * BC + thd_j];
        }
        li[thd_i] = l[row];
        mi[thd_i] = m[row];
        __syncthreads();

        /* gpu_QKDotAndScalar(sij, qi, kj, BR, BC, 1.0 / sqrt(double(d)), d) */
        sij[thd_i * BC + thd_j] = 0.0F;
        for(int idx = 0; idx < d; ++idx) {
            sij[thd_i * BC + thd_j] += qi[thd_i * d + idx] * kj[thd_j * d + idx];
        }
        float scalar = 1.0 / sqrt(double(d));
        sij[thd_i * BC + thd_j] *= scalar;
        // __syncthreads();

        /* gpu_RowMax(mij, sij, BR, BC) */
        mij[thd_i] = sij[thd_i * BC];
        for(int idx = 0; idx < BC; ++idx) {
            mij[thd_i] = max(mij[thd_i], sij[thd_i * BC + idx]);
        }
        // __syncthreads();

        /* gpu_MinusMaxAndExp(pij, sij, mij, BR, BC) */
        pij[thd_i * BC + thd_j] = exp(sij[thd_i * BC + thd_j] - mij[thd_i]);
        // __syncthreads();

        /* gpu_RowSum(lij, pij, BR, BC) */
        lij[thd_i] = 0.0F;
        for (int idx = 0; idx < BC; idx++) {
            lij[thd_i] += pij[thd_i * BC + idx];
        }
        // __syncthreads();

        /* gpu_UpdateMiLiOi(mi, li, oi, oi_tmp, mij, lij, pij, vj, BR, BC, d) */ 
        mi_new[thd_i] = max(mi[thd_i], mij[thd_i]);
        float coeff_old = exp(mi[thd_i] - mi_new[thd_i]);
        float coeff_cur = exp(mij[thd_i] - mi_new[thd_i]);
        li_new[thd_i] = coeff_old * li[thd_i] + coeff_cur * lij[thd_i];
        // __syncthreads();
        d_stride = d / BC;
        #pragma unroll
        for(int idx = 0; idx < d_stride; ++idx) {
            float pv = 0.0F;
            for(int t = 0; t < BC; ++t) {
                pv += pij[thd_i * BC + t] * vj[t * d + idx * BC + thd_j];
            }
            oi[thd_i * d + idx * BC + thd_j] = (li[thd_i] * coeff_old * oi[thd_i * d + idx * BC + thd_j] + coeff_cur * pv) / li_new[thd_i];
        }
        // __syncthreads();

        #pragma unroll
        for(int idx = 0; idx < d_stride; ++idx) {
            o[row * d + idx * BC + thd_j] = oi[thd_i * d + idx * BC + thd_j];
        }
        l[row] = li_new[thd_i];
        m[row] = mi_new[thd_i];
        // __syncthreads();
    }
    ```
    * matrix blocking: global matrixçš„åˆ‡å‰²ä¸»è¦åœ¨16è¡Œçš„for loopå…§å®Œæˆï¼Œé€™é‚Šå°æ‡‰åˆ°lab5 slides p.28çš„ç¬¬5è¡Œã€‚ä»¥å¤–å±¤loopçš„iä½œç‚ºæ¯æ¬¡è¦è¨ˆç®—çš„Qã€Kã€Vã€O (å’ŒCPU seq codeçš„flash attentionåšæ³•ä¸€æ¨£)ï¼Œä¸¦ä»¥å…§å±¤loopçš„ *j* ä½œç‚º*T~c~* çš„iterationã€‚
    * SRAM usage: 
        * d_strideç”¨æ–¼å°æ‡‰`d > 32`çš„æƒ…æ³ï¼Œæ˜¯memory coalescingçš„å±•ç¾ã€‚
        * 47-52è¡Œå°æ‡‰slides p.28ç¬¬6è¡Œï¼Œè² è²¬æŠŠ *k~j~* å’Œ *v~j~* å¾global memoryæ¬åˆ°shared memoryã€‚
        * 54-62è¡Œå°æ‡‰slides p.28ç¬¬7-8è¡Œï¼ŒæŠŠ*Q~i~*ã€*O~i~*ã€*â„“~i~*ã€*ğ‘š~i~* å¾global memoryæ¬è‡³shared memoryã€‚
        * 112-117è¡Œå°æ‡‰slides p.28ç¬¬12-13è¡Œï¼Œå¯«å›*O~i~*ã€*â„“~i~*ã€*ğ‘š~i~* è‡³global memoryã€‚
    * how intermediate results were calculated:
        * ä¸­é–“é‹ç®—çš„blocksæœ‰ç›¸æ‡‰çš„è¨»è§£ï¼Œæ–¹ä¾¿èˆ‡slidesçš„æ¼”ç®—æ³•åšå°ç…§ã€‚ä¸éåŸºæœ¬ä¸Šå°±æ˜¯æŠŠåŸæœ¬seq codeçš„functionæ¬éä¾†åšå¹³è¡ŒåŒ–ã€‚åƒæ˜¯`QKDotAndScalar()`å› ç‚ºæ²’æœ‰data dependencyå’Œrace conditionï¼Œå¯ä»¥åˆ©ç”¨æ‰€æœ‰threadsä¾†åŠ é€Ÿè¨ˆç®—ï¼›ä½†`RowMax()`å’Œ`RowSum()`å°±æœƒå› ç‚ºæœ‰å‰å…©è€…çš„å•é¡Œï¼Œä½¿å¾—åªèƒ½ç”¨threadIdx.yå»åšé‹ç®—ã€‚
2. Explain how matrices Q, K, and V are divided into blocks and processed in parallel.
    * å¦‚å‰é¡Œmatrix blockingèˆ‡SRAM usageæ‰€è¿°ï¼Œdivisionåœ¨main functionçš„å…©å±¤for loopå®Œæˆï¼Œè€Œå¹³è¡Œè™•ç†çš„æ–¹æ³•å‰‡æ˜¯è—‰d_strideä¾†æŠŠdataå¾global memoryå¹³è¡Œåœ°æ¬å…¥shared memoryã€‚
3. Describe how you chose the block sizes B_r and B_c and why.
    * ç¶­æŒèˆ‡seq codeä¸€æ¨£çš„32ã€‚ç”±æ–¼GPUçš„sharedMemPerBlockæœ‰é™ï¼Œç¶­æŒ32çš„æƒ…æ³ä¸‹è‹¥é‡åˆ°d=64çš„caseå·²ç¶“è®“shared memory sizeæ¥è¿‘æ¥µé™(41728 Bytes)ã€‚å› æ­¤ä¸ç‰¹åˆ¥èª¿é«˜ã€‚
4. Specify the configurations for CUDA kernel launches, such as the number of threads per block, shared memory allocation, and grid dimensions.
    * number of threads: 1024ã€‚MaxThreadsPerBlockå°±æ˜¯1024ï¼Œå› æ­¤å°±è¨­ç½®æˆ`dim3 threadsPerBlock(bc, br)`ã€‚
    * shared memory allocation: å¦‚ä¸Šé¢code partçš„29-42è¡Œã€‚åŸºæœ¬ä¸Šå°±æ˜¯æ ¹æ“šBRã€BCã€dçš„valueå»è¨­ç½®æ¯å€‹blockæ‰€éœ€çš„sizeã€‚
    * grid dimensions: `dim3 blocksPerGrid(1, tr)`ã€‚å› ç‚ºæ¯æ¬¡ *j* åœ¨å° *t~c~* åšiterationæ™‚ï¼Œéƒ½éœ€è¦ç”¨åˆ°å‰ä¸€æ¬¡çš„è¨ˆç®—çµæœï¼Œä¹Ÿå°±æ˜¯æœ‰data dependencyã€‚å› æ­¤æˆ‘çš„å¯¦ä½œæ‰æœƒåœ¨main functionè¨­ç½®2å±¤for loopï¼Œä¸¦æŠŠ *j* é€å…¥kernel functionä¾†å–å¾—æ­£ç¢ºçš„ *k~j~* å’Œ *v~j~* ã€‚
5. Justify your choices and how they relate to the blocking factors and the SRAM size.
    * ç†ç”±å·²åœ¨å‰é¡Œæ•˜è¿°ã€‚è€Œä»–å€‘èˆ‡blocking factorsã€SRAM sizeçš„é—œä¿‚å‰‡ä¸»è¦é«”ç¾åœ¨shared memory allocationã€‚
    * å› *k~j~* ã€ *v~j~* ã€ *q~i~* ã€ *o~i~* çš†é ˆè€ƒæ…®dåšallocationï¼Œè€Œdè¦ç­‰åˆ°runtimeåƒåˆ°testcaseæ‰æœƒçŸ¥é“ï¼Œç„¡æ³•äº‹å…ˆ`#define`ï¼Œæ‰€ä»¥æœƒåœ¨kernel functionå•Ÿå‹•æ™‚å‹•æ…‹åˆ†é…ã€‚
    * è€Œå…¶ä»–è¼”åŠ©è¨ˆç®—çš„shared memory blockså¦‚ *s~ij~* å‰‡å› ç‚ºBRå’ŒBCå¯ä»¥äº‹å…ˆ`#define`çš„é—œä¿‚ï¼Œå¯ä»¥éœæ…‹åˆ†é…æ–¼functionå…§éƒ¨ã€‚
    * å‰é¢ä¹Ÿæéï¼Œblocking factorséœ€è¦è€ƒæ…®åˆ°GPUçš„sharedMemPerBlockï¼Œè€Œåœ¨æ­¤æ¬¡hw4å¤šäº†ä¸€å€‹ä¸ç©©å®šçš„è®Šæ•¸dï¼Œå› æ­¤ç„¡æ³•åƒhw3èƒ½æ¯”è¼ƒéš¨æ„åœ°èª¿é«˜blocking factorä¾†å¢åŠ è¨ˆç®—çš„performanceã€‚

## Profiling Results
* é€™é‚Šæˆ‘é¸ç”¨t23é€™å€‹æ¸¬è³‡ã€‚è€Œkernel functionåªæœ‰`gpu_flash_attn()`ã€‚
    | Metric   | Min      | Max      | Avg    |
    | -------- | -------- | -------- | ------ |
    | achieved_occupancy | 0.497484 | 0.497778 | 0.497623 |
    | sm_efficiency      | 84.52%   | 90.70%   | 90.08%   |
    | shared_load_throughput  | 2517.7GB/s | 2772.1GB/s   | 2699.5GB/s |
    | shared_store_throughput | 178.56GB/s | 222.94GB/s   | 196.92GB/s |
    | gld_throughput | 22.859GB/s | 27.174GB/s | 23.840GB/s |
    | gst_throughput | 3.5315GB/s | 3.9333GB/s | 3.8247GB/s |
## Experiment & Analysis
* System Spec: apollo-gpu
* Optimization: 
    * testcase = t23, size = (B, N, d): (100, 4096, 32)
    * å› ç‚ºFlash Attentionæœ¬èº«çš„æ¦‚å¿µå°±æ˜¯ä½¿ç”¨SRAM blocké€²è¡ŒåŠ é€Ÿï¼Œå› æ­¤åŸæœ¬æ²’æœ‰å¯¦ä½œAttentionçš„ç‰ˆæœ¬ï¼Œä¾¿ä»¥CPU Sequential FlashAttnç‚ºbaselineã€‚
    * GPUä¸Šçš„threadsã€blocksã€BFçš†ç›¸åŒã€‚
    * å¾CPUåˆ°GPU shared memoryçš„åŠ é€Ÿå€ç‡é”åˆ°é©šäººçš„235.37å€ã€‚
    * æ¸›å°‘`__syncthreads()`æ•¸é‡å¾Œï¼ŒåŠ é€Ÿå¹…åº¦ä¸¦æ²’æœ‰åƒhw3æ˜é¡¯ã€‚å¯èƒ½æ˜¯å› ç‚ºtestcaseçš„sizeä¸¦ä¸å¤§ã€‚
    * ![hw4 Performance Optimization](https://hackmd.io/_uploads/ry6ZA-wSJe.png)

## Experience & Conclusion
* åœ¨æ’°å¯«kernel functionæ™‚ï¼Œæ›¾é‡éå®Œå…¨è·‘ä¸å‡ºä»»ä½•çµæœï¼Œä¹Ÿprintfä¸å‡ºä»»ä½•è³‡è¨Šçš„ç‹€æ³ã€‚ä½†é€™ç¨®ç‹€æ³åˆæ™‚å¥½æ™‚å£ï¼Œç•¶åˆå®Œå…¨ä¸çŸ¥é“åŸå› ç‚ºä½•ã€‚ç›´åˆ°çªç™¼å¥‡æƒ³cudauæ‡‰è©²æœ‰æä¾›æŸ¥çœ‹errorçš„APIï¼Œæ–¼æ˜¯æœå°‹å¾Œä½¿ç”¨çš„çµæœæ˜¯[CUDA Error: too many resources requested for launch](https://stackoverflow.com/questions/26201172/cuda-too-many-resources-requested-for-launch)ï¼Œç°¡å–®èªªå°±æ˜¯åœ¨compileéå¾Œkernel functionå…§ä½¿ç”¨çš„registeréå¤šï¼Œè®“GPUåœ¨runtimeæ™‚ç™¼ç”Ÿerrorç›´æ¥è¿”å›CPU processã€‚
    * æ–‡ç« æä¾›çš„è§£æ³•æ˜¯æ¸›å°‘threadsPerBlockçš„æ•¸é‡ï¼Œä½†é€™å¾ˆæ˜é¡¯æœƒé™ä½æ•ˆèƒ½ã€‚
    * å¾Œä¾†æˆ‘åˆè§€å¯Ÿåˆ°tcä¹‹é–“çš„computationæœ‰data dependencyï¼Œç„¡æ³•ä»¥blockIdx.xå»åšå¹³è¡ŒåŒ–ã€‚å› æ­¤æŠŠ`for(int j = 0; j < tc; ++j)`çš„iterationæ”¾åˆ°`main()`è£¡é¢ï¼Œæ²’æƒ³åˆ°é †ä¾¿è§£æ‰ä¸Šé¢é‡åˆ°çš„errorã€‚
    * è—‰æ­¤æˆ‘æ‰€å­¸åˆ°çš„æ˜¯åœ¨kernel functionæ‰€åŸ·è¡Œçš„äº‹é …ä¸è©²å¤ªå¤šã€‚æ¯å€‹äº‹é …çš„é‡å¯ä»¥å¤§ï¼Œä½†è¦è™•ç†çš„ç¨®é¡éå¤šçš„è©±å°±æœƒå°è‡´ç”¨äº†å¤ªå¤šregisterã€‚æ‡‰è©²è¦æŠŠæ¯å€‹åŠŸèƒ½åˆ†é–‹æ’°å¯«æœƒæ›´å¥½ã€‚
* é€éé€™æ¬¡ä½œæ¥­ï¼Œå†æ¬¡é«”æœƒåˆ°GPU codingçš„å›°é›£ä¹‹è™•(ä¸»è¦æ˜¯debugçš„éƒ¨åˆ†)ï¼Œä½†ä¹Ÿé«”æ„Ÿåˆ°GPUæ‰€èƒ½å¤ å¸¶ä¾†çš„åŠ é€Ÿæ•ˆæœæœ‰å¤šå¥½ã€‚
