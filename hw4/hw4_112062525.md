---
title: HW4 Flash Attention

---

# HW4 Flash Attention
> [name=蔡品棠, 112062525]
* 建議以hackmd開啟，程式碼區塊才能顯示行數。
## Implementation
1. Describe how you implemented the FlashAttention forward pass using CUDA. Mention the algorithm's key steps, such as matrix blocking, SRAM usage, and how intermediate results like scaling factors (ℓ and 𝑚) were calculated.
    ```cpp=
    int br = BR, bc = BC;
    int tr = N / br, tc = N / bc;
    dim3 blocksPerGrid(1, tr);
    dim3 threadsPerBlock(bc, br);
    size_t sizeof_kj_vj_qi_oi = (bc * d * 2 + br * d * 2) * sizeof(float);
    printf("size of kj_vj_qi_oi_oitmp = %lu\n", sizeof_kj_vj_qi_oi);
    printf("size of static shared mem = %lu\n", sizeof(float) * (BR * 6 + BR * BC * 2));

    double start, end;
    start = getTimeStamp();

    for (int i = 0; i < B; i++) {
        cudaMemset(l, 0x00, N * sizeof(float));
        cudaMemset(m, FLT_MIN, N * sizeof(float));
        
        for(int j = 0; j < tc; ++j)
            gpu_flash_attn<<<blocksPerGrid, threadsPerBlock, sizeof_kj_vj_qi_oi>>>(
                l, m, N, d, j,
                Q_gpu + (i * N * d), 
                K_gpu + (i * N * d), 
                V_gpu + (i * N * d), 
                O_gpu + (i * N * d)
            );
    }
    
    __global__ void gpu_flash_attn(float *l, float *m, int N, int d, int j,
                                   float *q, float *k, float *v, float *o
                                   ) {
        extern __shared__ float s_mem[];
        float *kj = s_mem;
        float *vj = kj + BC * d;
        float *qi = vj + BC * d;
        float *oi = qi + BR * d;

        __shared__ float li[BR];
        __shared__ float mi[BR];
        __shared__ float sij[BR * BC];
        __shared__ float pij[BR * BC];
        __shared__ float mij[BR];
        __shared__ float lij[BR];
        __shared__ float mi_new[BR];
        __shared__ float li_new[BR];

        int thd_i = threadIdx.y;
        int thd_j = threadIdx.x;

        int row = blockIdx.y * BR + thd_i;

        int d_stride = d / BR;
        #pragma unroll
        for(int idx = 0; idx < d_stride; ++idx) {    
            kj[thd_j * d + idx * BR + thd_i] = k[j * BC * d + thd_j * d + idx * BR + thd_i];
            vj[thd_j * d + idx * BR + thd_i] = v[j * BC * d + thd_j * d + idx * BR + thd_i];
        }

        d_stride = d / BC;
        #pragma unroll
        for(int idx = 0; idx < d_stride; ++idx) {
            qi[thd_i * d + idx * BC + thd_j] = q[row * d + idx * BC + thd_j];
            oi[thd_i * d + idx * BC + thd_j] = o[row * d + idx * BC + thd_j];
        }
        li[thd_i] = l[row];
        mi[thd_i] = m[row];
        __syncthreads();

        /* gpu_QKDotAndScalar(sij, qi, kj, BR, BC, 1.0 / sqrt(double(d)), d) */
        sij[thd_i * BC + thd_j] = 0.0F;
        for(int idx = 0; idx < d; ++idx) {
            sij[thd_i * BC + thd_j] += qi[thd_i * d + idx] * kj[thd_j * d + idx];
        }
        float scalar = 1.0 / sqrt(double(d));
        sij[thd_i * BC + thd_j] *= scalar;
        // __syncthreads();

        /* gpu_RowMax(mij, sij, BR, BC) */
        mij[thd_i] = sij[thd_i * BC];
        for(int idx = 0; idx < BC; ++idx) {
            mij[thd_i] = max(mij[thd_i], sij[thd_i * BC + idx]);
        }
        // __syncthreads();

        /* gpu_MinusMaxAndExp(pij, sij, mij, BR, BC) */
        pij[thd_i * BC + thd_j] = exp(sij[thd_i * BC + thd_j] - mij[thd_i]);
        // __syncthreads();

        /* gpu_RowSum(lij, pij, BR, BC) */
        lij[thd_i] = 0.0F;
        for (int idx = 0; idx < BC; idx++) {
            lij[thd_i] += pij[thd_i * BC + idx];
        }
        // __syncthreads();

        /* gpu_UpdateMiLiOi(mi, li, oi, oi_tmp, mij, lij, pij, vj, BR, BC, d) */ 
        mi_new[thd_i] = max(mi[thd_i], mij[thd_i]);
        float coeff_old = exp(mi[thd_i] - mi_new[thd_i]);
        float coeff_cur = exp(mij[thd_i] - mi_new[thd_i]);
        li_new[thd_i] = coeff_old * li[thd_i] + coeff_cur * lij[thd_i];
        // __syncthreads();
        d_stride = d / BC;
        #pragma unroll
        for(int idx = 0; idx < d_stride; ++idx) {
            float pv = 0.0F;
            for(int t = 0; t < BC; ++t) {
                pv += pij[thd_i * BC + t] * vj[t * d + idx * BC + thd_j];
            }
            oi[thd_i * d + idx * BC + thd_j] = (li[thd_i] * coeff_old * oi[thd_i * d + idx * BC + thd_j] + coeff_cur * pv) / li_new[thd_i];
        }
        // __syncthreads();

        #pragma unroll
        for(int idx = 0; idx < d_stride; ++idx) {
            o[row * d + idx * BC + thd_j] = oi[thd_i * d + idx * BC + thd_j];
        }
        l[row] = li_new[thd_i];
        m[row] = mi_new[thd_i];
        // __syncthreads();
    }
    ```
    * matrix blocking: global matrix的切割主要在16行的for loop內完成，這邊對應到lab5 slides p.28的第5行。以外層loop的i作為每次要計算的Q、K、V、O (和CPU seq code的flash attention做法一樣)，並以內層loop的 *j* 作為*T~c~* 的iteration。
    * SRAM usage: 
        * d_stride用於對應`d > 32`的情況，是memory coalescing的展現。
        * 47-52行對應slides p.28第6行，負責把 *k~j~* 和 *v~j~* 從global memory搬到shared memory。
        * 54-62行對應slides p.28第7-8行，把*Q~i~*、*O~i~*、*ℓ~i~*、*𝑚~i~* 從global memory搬至shared memory。
        * 112-117行對應slides p.28第12-13行，寫回*O~i~*、*ℓ~i~*、*𝑚~i~* 至global memory。
    * how intermediate results were calculated:
        * 中間運算的blocks有相應的註解，方便與slides的演算法做對照。不過基本上就是把原本seq code的function搬過來做平行化。像是`QKDotAndScalar()`因為沒有data dependency和race condition，可以利用所有threads來加速計算；但`RowMax()`和`RowSum()`就會因為有前兩者的問題，使得只能用threadIdx.y去做運算。
2. Explain how matrices Q, K, and V are divided into blocks and processed in parallel.
    * 如前題matrix blocking與SRAM usage所述，division在main function的兩層for loop完成，而平行處理的方法則是藉d_stride來把data從global memory平行地搬入shared memory。
3. Describe how you chose the block sizes B_r and B_c and why.
    * 維持與seq code一樣的32。由於GPU的sharedMemPerBlock有限，維持32的情況下若遇到d=64的case已經讓shared memory size接近極限(41728 Bytes)。因此不特別調高。
4. Specify the configurations for CUDA kernel launches, such as the number of threads per block, shared memory allocation, and grid dimensions.
    * number of threads: 1024。MaxThreadsPerBlock就是1024，因此就設置成`dim3 threadsPerBlock(bc, br)`。
    * shared memory allocation: 如上面code part的29-42行。基本上就是根據BR、BC、d的value去設置每個block所需的size。
    * grid dimensions: `dim3 blocksPerGrid(1, tr)`。因為每次 *j* 在對 *t~c~* 做iteration時，都需要用到前一次的計算結果，也就是有data dependency。因此我的實作才會在main function設置2層for loop，並把 *j* 送入kernel function來取得正確的 *k~j~* 和 *v~j~* 。
5. Justify your choices and how they relate to the blocking factors and the SRAM size.
    * 理由已在前題敘述。而他們與blocking factors、SRAM size的關係則主要體現在shared memory allocation。
    * 因*k~j~* 、 *v~j~* 、 *q~i~* 、 *o~i~* 皆須考慮d做allocation，而d要等到runtime吃到testcase才會知道，無法事先`#define`，所以會在kernel function啟動時動態分配。
    * 而其他輔助計算的shared memory blocks如 *s~ij~* 則因為BR和BC可以事先`#define`的關係，可以靜態分配於function內部。
    * 前面也提過，blocking factors需要考慮到GPU的sharedMemPerBlock，而在此次hw4多了一個不穩定的變數d，因此無法像hw3能比較隨意地調高blocking factor來增加計算的performance。

## Profiling Results
* 這邊我選用t23這個測資。而kernel function只有`gpu_flash_attn()`。
    | Metric   | Min      | Max      | Avg    |
    | -------- | -------- | -------- | ------ |
    | achieved_occupancy | 0.497484 | 0.497778 | 0.497623 |
    | sm_efficiency      | 84.52%   | 90.70%   | 90.08%   |
    | shared_load_throughput  | 2517.7GB/s | 2772.1GB/s   | 2699.5GB/s |
    | shared_store_throughput | 178.56GB/s | 222.94GB/s   | 196.92GB/s |
    | gld_throughput | 22.859GB/s | 27.174GB/s | 23.840GB/s |
    | gst_throughput | 3.5315GB/s | 3.9333GB/s | 3.8247GB/s |
## Experiment & Analysis
* System Spec: apollo-gpu
* Optimization: 
    * testcase = t23, size = (B, N, d): (100, 4096, 32)
    * 因為Flash Attention本身的概念就是使用SRAM block進行加速，因此原本沒有實作Attention的版本，便以CPU Sequential FlashAttn為baseline。
    * GPU上的threads、blocks、BF皆相同。
    * 從CPU到GPU shared memory的加速倍率達到驚人的235.37倍。
    * 減少`__syncthreads()`數量後，加速幅度並沒有像hw3明顯。可能是因為testcase的size並不大。
    * ![hw4 Performance Optimization](https://hackmd.io/_uploads/ry6ZA-wSJe.png)

## Experience & Conclusion
* 在撰寫kernel function時，曾遇過完全跑不出任何結果，也printf不出任何資訊的狀況。但這種狀況又時好時壞，當初完全不知道原因為何。直到突發奇想cudau應該有提供查看error的API，於是搜尋後使用的結果是[CUDA Error: too many resources requested for launch](https://stackoverflow.com/questions/26201172/cuda-too-many-resources-requested-for-launch)，簡單說就是在compile過後kernel function內使用的register過多，讓GPU在runtime時發生error直接返回CPU process。
    * 文章提供的解法是減少threadsPerBlock的數量，但這很明顯會降低效能。
    * 後來我又觀察到tc之間的computation有data dependency，無法以blockIdx.x去做平行化。因此把`for(int j = 0; j < tc; ++j)`的iteration放到`main()`裡面，沒想到順便解掉上面遇到的error。
    * 藉此我所學到的是在kernel function所執行的事項不該太多。每個事項的量可以大，但要處理的種類過多的話就會導致用了太多register。應該要把每個功能分開撰寫會更好。
* 透過這次作業，再次體會到GPU coding的困難之處(主要是debug的部分)，但也體感到GPU所能夠帶來的加速效果有多好。
